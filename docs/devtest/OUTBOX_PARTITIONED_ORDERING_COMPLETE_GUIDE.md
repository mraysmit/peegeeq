# PeeGeeQ Outbox: Partitioned Ordering - Complete Guide

**Author**: Mark A Ray-Smith, Cityline Ltd  
**Date**: October 2025  
**Status**: Production Ready  
**Version**: 1.0

---

## Executive Summary

**Key Finding**: PeeGeeQ outbox provides the **infrastructure for partitioned ordering** through the `message_group` column, but **does NOT automatically enforce strict per-partition processing order**. With multiple concurrent consumers (a key PeeGeeQ feature), messages from the same partition may be processed out of order unless you implement explicit application-level routing.

### What's Guaranteed vs. What Requires Configuration

| Feature | Guaranteed | Details |
|---------|-----------|---------|
| Message Group Column | ✅ | Stored in outbox table, indexed |
| Producer API | ✅ | All send methods accept messageGroup parameter |
| Consumer Retrieval | ✅ | Message group returned with each message |
| **Storage Order** (per group) | ✅ | Messages with same group stored in `created_at ASC` order |
| **Processing Order** (per group) | ❌ | **NOT guaranteed with concurrent consumers** - requires application-level routing |
| Automatic Partition Assignment | ❌ | Must implement in application |
| Partition-Aware Consumer Distribution | ❌ | No built-in partition routing |
| Automatic Rebalancing | ❌ | No failover/rebalancing logic |

---

## Ordering Guarantees

### Storage Order (Database Level)

**Guarantee**: All messages are stored in `created_at ASC` order.

**Per-Group Storage**: Messages with the same `message_group` are stored in order.

**Applies To**: Entire topic, all message groups.

**Important**: This is a **database guarantee only**. Storage order does NOT automatically translate to processing order.

### Processing Order (Application Level)

**Critical Limitation**: With multiple concurrent consumers (PeeGeeQ's key feature), messages are **NOT processed in storage order** unless you implement explicit routing.

**Why**: The consumer query uses `FOR UPDATE SKIP LOCKED`, which allows multiple consumers to grab different messages concurrently. This maximizes throughput but breaks ordering guarantees.

**Per-Group Processing**: Messages with the same `message_group` are **NOT guaranteed** to be processed in order when multiple consumers process the same group concurrently.

**Solution**: To guarantee strict per-group processing order, you must:
1. Route all messages from the same group to a single consumer, OR
2. Implement application-level partition queues (Pattern 3), OR
3. Use a single consumer for the entire topic (sacrifices parallelism)

---

## Three Patterns for Strict Ordering

### Pattern 1: Single Consumer Per Partition ⭐ Recommended

**Use When**: < 100 partitions, throughput < 10,000 msg/s

```java
// Producer: Set partition key
producer.send(payload, headers, correlationId, "customer-123");

// Consumer: Single consumer processes all messages
MessageConsumer<OrderEvent> consumer = factory.createConsumer(topic, OrderEvent.class);
consumer.subscribe(message -> {
    OrderEvent event = message.getPayload();
    String partition = message.getMessageGroup();
    logger.info("Processing order for customer {}", partition);
    return processOrder(event);
});
consumer.start();
```

**Guarantees**: ✅ Strict ordering per partition  
**Throughput**: ~1,000 msg/s per consumer  
**Complexity**: Low  
**Scalability**: O(partitions)

---

### Pattern 2: Consumer Group with Partition Filter

**Use When**: 10-100 partitions, throughput 1,000-10,000 msg/s

```java
// Producer: Set partition key
producer.send(payload, headers, correlationId, "security-456");

// Consumer: Filter by partition
ConsumerGroup<TradeEvent> group = factory.createConsumerGroup(
    "settlement", topic, TradeEvent.class);

group.addConsumer("settlement-1", message -> {
    TradeEvent trade = message.getPayload();
    return processSettlement(trade);
}, message -> message.getMessageGroup().startsWith("security"));

group.start();
```

**Guarantees**: ✅ Strict ordering per partition  
**Throughput**: ~5,000 msg/s total  
**Complexity**: Medium  
**Scalability**: O(partitions)

---

### Pattern 3: Application-Level Partitioning

**Use When**: > 100 partitions, dynamic partitions, complex logic

```java
public class PartitionAwareConsumer {
    private final Map<String, LinkedBlockingQueue<Message<?>>> partitionQueues 
        = new ConcurrentHashMap<>();
    private final ExecutorService partitionExecutors = Executors.newCachedThreadPool();
    
    public void startPartitionedConsumer(MessageConsumer<OrderEvent> consumer) {
        consumer.subscribe(message -> {
            String partition = message.getMessageGroup();
            
            // Route to partition-specific queue
            partitionQueues.computeIfAbsent(partition, k -> 
                new LinkedBlockingQueue<>()).offer(message);
            
            // Process partition queue (single thread per partition)
            processPartitionQueue(partition);
            
            return CompletableFuture.completedFuture(null);
        });
    }
    
    private void processPartitionQueue(String partition) {
        partitionExecutors.execute(() -> {
            LinkedBlockingQueue<Message<?>> queue = partitionQueues.get(partition);
            while (!Thread.currentThread().isInterrupted()) {
                try {
                    Message<?> message = queue.poll(1, TimeUnit.SECONDS);
                    if (message != null) {
                        processMessage(message);
                    }
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
            }
        });
    }
}
```

**Guarantees**: ✅ Strict ordering per partition  
**Throughput**: ~5,000 msg/s total  
**Complexity**: High  
**Scalability**: O(partitions)

---

## Database Schema

### Outbox Table Structure

```sql
CREATE TABLE outbox (
    id BIGSERIAL PRIMARY KEY,
    topic VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE,
    status VARCHAR(50) DEFAULT 'PENDING',
    headers JSONB DEFAULT '{}',
    correlation_id VARCHAR(255),
    message_group VARCHAR(255),           -- Partition key
    priority INT DEFAULT 5,
    retry_count INT DEFAULT 0,
    max_retries INT DEFAULT 3
);

-- Indexes for ordering
CREATE INDEX idx_outbox_topic_status ON outbox(topic, status);
CREATE INDEX idx_outbox_status_created_at ON outbox(status, created_at);
CREATE INDEX idx_outbox_message_group ON outbox(message_group) 
    WHERE message_group IS NOT NULL;
```

### Current Consumer Query

```java
String sql = """
    UPDATE outbox
    SET status = 'PROCESSING', processed_at = $1
    WHERE id IN (
        SELECT id FROM outbox
        WHERE topic = $2 AND status = 'PENDING'
        ORDER BY created_at ASC
        LIMIT $3
        FOR UPDATE SKIP LOCKED
    )
    RETURNING id, payload, headers, correlation_id, message_group, created_at
    """;
```

**Key Points**:
- Orders by `created_at ASC` (FIFO)
- Does NOT filter by `message_group`
- Uses `FOR UPDATE SKIP LOCKED` for concurrent safety
- Returns `message_group` for consumer use

---

## Use Case Scenarios

### Scenario 1: E-Commerce Order Processing

**Requirements**:
- Orders from same customer must be processed in order
- Different customers can be processed in parallel
- High throughput (10,000+ orders/day)

**Solution**: Single Consumer Per Partition
```java
producer.send(order, headers, correlationId, order.getCustomerId());
```

**Guarantees**: ✅ Order 1 → Order 2 → Order 3 (per customer)

---

### Scenario 2: Financial Trade Settlement

**Requirements**:
- Trades for same security must be settled in order
- Different securities can be settled in parallel
- Strict ordering critical for regulatory compliance

**Solution**: Consumer Group with Partition Filter
```java
producer.send(trade, headers, correlationId, trade.getSecurityId());
```

**Guarantees**: ✅ Strict ordering per security

---

### Scenario 3: Real-Time Analytics

**Requirements**:
- Process all events as fast as possible
- Ordering not critical
- Maximize throughput

**Solution**: Multiple Consumers (No Partitioning)
```java
producer.send(event, headers, correlationId, null);
```

**Guarantees**: ✅ Maximum throughput (10x single consumer)

---

### Scenario 4: Saga Orchestration

**Requirements**:
- Saga steps must execute in order
- Multiple sagas can run in parallel
- Compensation on failure

**Solution**: Application-Level Partitioning
```java
producer.send(sagaStep, headers, correlationId, saga.getId());
```

**Guarantees**: ✅ Saga steps in order

---

## Decision Matrix

### Do You Need Strict Ordering?

```
Question 1: Must messages be processed in exact order?
├─ YES → Question 2
└─ NO → Use Multiple Consumers (High Throughput)

Question 2: Is ordering required per customer/entity or globally?
├─ Per Entity → Question 3
├─ Globally → Use Single Consumer (Low Throughput)
└─ Not Sure → Consult architect

Question 3: How many entities (partitions) do you have?
├─ < 100 → Pattern 1: Single Consumer Per Partition
├─ 10-100 → Pattern 2: Consumer Group with Partition Filter
├─ > 100 → Pattern 3: Application-Level Partitioning
└─ Dynamic → Pattern 3: Application-Level Partitioning
```

---

## Performance Characteristics

| Pattern | Throughput | Latency | Ordering | Complexity |
|---------|-----------|---------|----------|-----------|
| Multiple Consumers | 10,000+ msg/s | <5ms | None | Low |
| Single Consumer/Partition | 1,000 msg/s | <10ms | Strict | Low |
| Consumer Group + Filter | 5,000 msg/s | <10ms | Strict | Medium |
| Application-Level | 5,000 msg/s | 10-50ms | Strict | High |

---

## Best Practices

### ✅ DO

1. **Define Partition Key**: Choose entity that requires ordering
   - Customer ID, Security ID, Account ID, Saga ID

2. **Document Strategy**: Record partition key and rationale
   - Why this key? How many partitions? Expected throughput?

3. **Set Message Group**: Always populate message_group in producer
   - `producer.send(payload, headers, correlationId, partitionKey)`

4. **Monitor Ordering**: Verify messages processed in order
   - Track sequence numbers per partition
   - Alert on out-of-order messages

5. **Test Thoroughly**: Load test with concurrent sends
   - Verify ordering under load
   - Test consumer failures and recovery

### ❌ DON'T

1. **Assume Ordering with Multiple Consumers**: Requires single consumer per partition
2. **Ignore Message Group**: Set it even if not using partitioning
3. **Mix Ordered and Unordered**: Choose pattern and stick with it
4. **Forget Monitoring**: Ordering issues discovered in production are expensive
5. **Ignore Partition Imbalance**: Uneven distribution causes bottlenecks

---

## Implementation Checklist

### Pre-Implementation

- [ ] Identify ordering requirement (YES/NO)
- [ ] Determine if ordering per entity or global
- [ ] Identify partition key (customer, security, account, etc.)
- [ ] Estimate partition count
- [ ] Define throughput requirement
- [ ] Choose pattern (1, 2, or 3)
- [ ] Document architecture

### Implementation

- [ ] Update producer to set message_group
- [ ] Implement consumer for chosen pattern
- [ ] Add partition key extraction logic
- [ ] Implement message handler
- [ ] Add error handling
- [ ] Add logging and monitoring

### Testing

- [ ] Unit tests for partition key extraction
- [ ] Integration tests for ordering
- [ ] Load tests with concurrent sends
- [ ] Failure tests (consumer, producer, database)
- [ ] Performance tests (throughput, latency)

### Deployment

- [ ] Code review and approval
- [ ] Staging deployment
- [ ] Production deployment
- [ ] Monitor closely
- [ ] Verify ordering
- [ ] Check metrics and alerts

### Post-Deployment

- [ ] Daily monitoring
- [ ] Weekly performance review
- [ ] Monthly optimization
- [ ] Document lessons learned

---

## Monitoring and Observability

### Key Metrics Per Partition

```
1. Message Count
   - Total messages per partition
   - Pending messages per partition
   - Failed messages per partition

2. Latency
   - Time in queue per partition
   - Processing time per partition
   - End-to-end latency per partition

3. Throughput
   - Messages/sec per partition
   - Bytes/sec per partition
   - Success rate per partition

4. Ordering
   - Out-of-order messages (should be 0)
   - Sequence gaps (should be 0)
   - Reordering events (should be 0)
```

### Alerting Rules

- Alert if out-of-order messages > 0
- Alert if partition queue depth > threshold
- Alert if partition latency > SLA
- Alert if partition processing fails
- Alert if partition consumer down

---

## Common Mistakes to Avoid

### ❌ Mistake 1: Assuming Ordering with Multiple Consumers

```java
// WRONG: Multiple consumers, no partition awareness
ConsumerGroup<Order> group = factory.createConsumerGroup(
    "orders", topic, Order.class);
group.addConsumer("c1", this::processOrder);
group.addConsumer("c2", this::processOrder);
// Result: Orders from same customer may be out of order!
```

### ❌ Mistake 2: Not Setting Message Group

```java
// WRONG: No partition key
producer.send(order, headers, correlationId, null);
// Result: No way to enforce ordering
```

### ❌ Mistake 3: Ignoring Partition Imbalance

```
1000 orders for customer-1
1 order for customer-2
Result: Customer-1 consumer overloaded
```

### ❌ Mistake 4: No Monitoring

```
No visibility into ordering
Result: Discover ordering issues in production
```

---

## Troubleshooting

### Problem: Messages Out of Order

**Diagnosis**:
1. Check if multiple consumers processing same partition
2. Verify message_group is set correctly
3. Check consumer logs for concurrent processing
4. Monitor partition queue depth

**Solution**:
- Use single consumer per partition
- Implement application-level ordering
- Add ordering verification tests

### Problem: High Latency

**Diagnosis**:
1. Check partition distribution
2. Monitor consumer processing time
3. Check database query performance
4. Verify network latency

**Solution**:
- Add more consumers (if no ordering required)
- Optimize processing logic
- Increase batch size
- Add database indexes

---

## Future Enhancements

### Phase 2 (Recommended)

- Partition-aware consumer query optimization
- Consumer group partition assignment API
- Per-partition metrics and monitoring
- Partition rebalancing support

### Phase 3 (Future)

- Ordered consumer group variant
- Automatic partition assignment
- Partition rebalancing protocol
- Kafka-compatible partition API

---

## Summary

| Aspect | Current | Recommended |
|--------|---------|-------------|
| **Message Group Storage** | ✅ Supported | Use for logical partitioning |
| **Per-Group Ordering** | ⚠️ Partial | Single consumer per group |
| **Automatic Partitioning** | ❌ Not supported | Implement in application |
| **Rebalancing** | ❌ Not supported | Use external service discovery |
| **Strict Ordering** | ✅ Possible | With single consumer per group |

**Conclusion**: PeeGeeQ outbox supports partitioned ordering through message groups, but requires explicit application-level configuration to guarantee strict ordering. For production systems requiring strict per-partition ordering, implement one of the three patterns based on your partition count and throughput requirements.

---

## Ordering Verification Test

```java
@Test
void testPartitionedOrdering() {
    Map<String, List<Integer>> partitionSequences = new ConcurrentHashMap<>();

    consumer.subscribe(message -> {
        OrderEvent event = message.getPayload();
        String partition = message.getMessageGroup();
        int sequence = event.getSequence();

        // Track sequence per partition
        partitionSequences.computeIfAbsent(partition, k ->
            Collections.synchronizedList(new ArrayList<>()))
            .add(sequence);

        return CompletableFuture.completedFuture(null);
    });

    // Send messages with different partitions
    for (int i = 1; i <= 100; i++) {
        String partition = "customer-" + (i % 10);
        OrderEvent event = new OrderEvent("order-" + i, i, partition);
        producer.send(event, null, null, partition).join();
    }

    // Verify ordering per partition
    for (List<Integer> sequences : partitionSequences.values()) {
        for (int i = 1; i < sequences.size(); i++) {
            assertTrue(sequences.get(i) > sequences.get(i-1),
                "Sequence out of order: " + sequences);
        }
    }
}
```

---

## Producer Implementation Details

### Send with Message Group

```java
public CompletableFuture<Void> send(T payload, Map<String, String> headers,
                                    String correlationId, String messageGroup) {
    return sendInternal(payload, headers, correlationId, messageGroup);
}

// Internal implementation
String sql = """
    INSERT INTO outbox (topic, payload, headers, correlation_id, message_group,
                       created_at, status)
    VALUES ($1, $2::jsonb, $3::jsonb, $4, $5, $6, 'PENDING')
    """;

Tuple params = Tuple.of(
    topic,
    payloadJson,
    headersJson,
    finalCorrelationId,
    messageGroup,  // Partition key
    OffsetDateTime.now()
);
```

### Transactional Send with Message Group

```java
public CompletableFuture<Void> sendWithTransaction(T payload,
                                                   Map<String, String> headers,
                                                   String correlationId,
                                                   String messageGroup,
                                                   SqlConnection connection) {
    String sql = """
        INSERT INTO outbox (topic, payload, headers, correlation_id, message_group,
                           created_at, status)
        VALUES ($1, $2::jsonb, $3::jsonb, $4, $5, $6, 'PENDING')
        """;

    Tuple params = Tuple.of(
        topic,
        toJsonObject(payload),
        headersToJsonObject(headers),
        correlationId != null ? correlationId : UUID.randomUUID().toString(),
        messageGroup,
        OffsetDateTime.now()
    );

    return connection.preparedQuery(sql)
        .execute(params)
        .mapEmpty()
        .toCompletionStage()
        .toCompletableFuture();
}
```

---

## Consumer Implementation Details

### Single Consumer Pattern

```java
public void startSingleConsumer() {
    MessageConsumer<OrderEvent> consumer = factory.createConsumer(topic, OrderEvent.class);

    consumer.subscribe(message -> {
        OrderEvent event = message.getPayload();
        String partition = message.getMessageGroup();

        logger.info("Processing order {} for partition {}",
            event.getOrderId(), partition);

        try {
            // Process in strict order - only one consumer
            return processOrder(event)
                .thenApply(result -> {
                    logger.info("Successfully processed order {} for partition {}",
                        event.getOrderId(), partition);
                    return null;
                })
                .exceptionally(ex -> {
                    logger.error("Failed to process order {} for partition {}: {}",
                        event.getOrderId(), partition, ex.getMessage());
                    throw new RuntimeException(ex);
                });
        } catch (Exception e) {
            logger.error("Error processing order: {}", e.getMessage());
            return CompletableFuture.failedFuture(e);
        }
    });

    consumer.start();
}
```

### Consumer Group with Partition Filter

```java
public void startConsumerGroupWithFilter() {
    ConsumerGroup<OrderEvent> group = factory.createConsumerGroup(
        "order-processors", topic, OrderEvent.class);

    // Add consumers with partition-specific filters
    String[] partitions = {"customer-1", "customer-2", "customer-3"};

    for (int i = 0; i < partitions.length; i++) {
        final String partition = partitions[i];
        final int consumerId = i;

        group.addConsumer("consumer-" + consumerId,
            message -> {
                OrderEvent event = message.getPayload();
                logger.info("Consumer {} processing order for partition {}",
                    consumerId, partition);
                return processOrder(event);
            },
            message -> partition.equals(message.getMessageGroup())
        );
    }

    group.start();
}
```

---

## Requirements Checklist

### Before Implementation

- [ ] Identify partition key (customer, security, account, saga, etc.)
- [ ] Estimate partition count (< 100, 10-100, > 100)
- [ ] Define throughput requirement (msg/s)
- [ ] Define latency SLA (ms)
- [ ] Choose pattern (1, 2, or 3)
- [ ] Document architecture and rationale
- [ ] Identify monitoring requirements
- [ ] Plan testing strategy

### During Implementation

- [ ] Producer sets message_group correctly
- [ ] Consumer retrieves message_group
- [ ] Partition key extraction logic tested
- [ ] Error handling implemented
- [ ] Logging and monitoring added
- [ ] Code reviewed and approved

### After Implementation

- [ ] Unit tests passing
- [ ] Integration tests passing
- [ ] Load tests completed
- [ ] Failure tests completed
- [ ] Performance meets SLA
- [ ] Monitoring working
- [ ] Documentation updated
- [ ] Team trained

---

## Appendix A: Technical Deep Dive - How Other Systems Solve Ordering

This appendix explores how Kafka and Apache Pulsar solve the per-partition ordering problem, and discusses potential future enhancements for PeeGeeQ.

### The Core Problem

PeeGeeQ's current consumer query uses `FOR UPDATE SKIP LOCKED`, which allows multiple consumers to grab different messages concurrently. This maximizes throughput but breaks per-partition ordering guarantees. How do other systems handle this?

---

## Kafka's Solution: Partition Assignment

### Architecture

Kafka solves ordering through **exclusive partition assignment** enforced by a coordinator:

1. **Producer**: Sends to a partition based on the message key (or round-robin if no key)
2. **Consumer Group**: Multiple consumers subscribe to the same topic
3. **Coordinator**: A broker acts as coordinator and assigns partitions to consumers
   - Consumer 1 → Partitions [0, 2, 4]
   - Consumer 2 → Partitions [1, 3, 6]
4. **Ordering Guarantee**: Since each partition goes to exactly one consumer, and messages within a partition are ordered, strict per-partition ordering is guaranteed
5. **Rebalancing**: When a consumer joins/leaves, the coordinator reassigns partitions

### Why This Works

- **Stateless consumers**: Consumers don't need to coordinate with each other, just follow the assignment
- **Offset tracking**: Each consumer tracks its offset per partition, so it knows where it left off
- **Rebalancing protocol**: Well-defined protocol for handling consumer failures/additions
- **Scalability**: Trivial to add more consumers - just rebalance

### The Rebalancing Problem

Kafka's approach has a known limitation: when a consumer fails or joins, **all consumers in the group stop and rejoin**. This causes:

- **Stop-the-world pause**: Even healthy consumers pause processing
- **Latency spike**: All consumers wait for rebalancing to complete
- **Complexity**: Rebalancing protocol is complex and can be a source of bugs
- **Operational overhead**: Requires careful tuning of heartbeat timeouts and session timeouts

---

## Apache Pulsar's Solution: Broker-Enforced Assignment

### Architecture

Pulsar sidesteps Kafka's rebalancing problem through a fundamentally different architecture:

**Key difference**: Pulsar separates **storage** (BookKeeper) from **serving** (brokers). This separation enables a cleaner solution.

1. **Partitioned Topics**: Like Kafka, messages are partitioned by key
2. **Broker-Enforced Assignment**: Each partition is assigned to one consumer, but the assignment is **enforced by the broker**, not a separate coordinator
3. **Stateless Consumers**: Consumers don't coordinate with each other; the broker manages assignments
4. **Immediate Failover**: When a consumer fails, the broker immediately reassigns that partition to another consumer

### How Pulsar Avoids Rebalancing Pain

**Kafka's rebalancing**:
- Coordinator detects consumer failure (via heartbeat timeout)
- Triggers rebalancing
- **All consumers stop** and rejoin
- New assignment computed
- All consumers resume

**Pulsar's approach**:
- Broker detects consumer failure
- **Immediately reassigns that partition to another consumer**
- No stop-the-world pause
- Other consumers keep processing unaffected
- Failed consumer's offset is preserved in BookKeeper

### Why This Works

1. **Broker-side state**: Brokers maintain partition-to-consumer mapping, not a separate coordinator
2. **Stateless consumers**: Consumers don't need to know about each other or coordinate
3. **Immediate failover**: Broker detects failure and reassigns instantly
4. **Offset management**: BookKeeper stores offsets independently, so any consumer can resume from where the previous one left off
5. **No group coordination**: Failure of one consumer doesn't affect others

### The Architecture Difference

| Aspect | Kafka | Pulsar |
|--------|-------|--------|
| Storage | Brokers store data | BookKeeper (separate) |
| Consumer Coordination | Via coordinator | Via broker |
| Rebalancing | Group-wide stop-the-world | Per-partition immediate reassignment |
| Consumer Failure Impact | All consumers pause | Only affected partition reassigned |
| Offset Storage | Kafka topic | BookKeeper |
| Complexity | Moderate | Higher (separate storage layer) |

---

## Potential Future Enhancements for PeeGeeQ

### Option 1: Kafka-Like Partition Assignment

Implement Kafka's approach in PeeGeeQ:

**Requirements**:
1. **Consumer Registry**: Track which consumer instances are active
   - `consumer_group_members` table with heartbeat mechanism
   - Detect failures via heartbeat timeout

2. **Partition Assignment Strategy**:
   - Range-based: Consumer 1 gets groups A-M, Consumer 2 gets N-Z
   - Round-robin: Distribute groups evenly
   - Sticky: Minimize reassignment on rebalancing

3. **Assignment Storage**:
   ```sql
   CREATE TABLE consumer_assignments (
       group_id VARCHAR(255),
       consumer_id VARCHAR(255),
       message_group VARCHAR(255),
       assigned_at TIMESTAMP WITH TIME ZONE,
       PRIMARY KEY (group_id, consumer_id, message_group)
   );
   ```

4. **Modified Consumer Query**:
   ```sql
   SELECT id FROM outbox
   WHERE topic = $1
     AND status = 'PENDING'
     AND message_group IN (
       SELECT message_group FROM consumer_assignments
       WHERE group_id = $2 AND consumer_id = $3
     )
   ORDER BY created_at ASC
   LIMIT $4
   FOR UPDATE SKIP LOCKED
   ```

5. **Rebalancing Trigger**:
   - Consumer heartbeat fails → remove from registry
   - New consumer joins → trigger rebalancing
   - Reassign groups to active consumers

**Advantages**:
- Familiar model (Kafka-like)
- Strict per-group ordering
- Parallelism across groups

**Disadvantages**:
- Stop-the-world rebalancing pauses all consumers
- Operational complexity
- Requires heartbeat/liveness detection

---

### Option 2: Pulsar-Like Broker-Enforced Assignment

Implement Pulsar's approach in PeeGeeQ:

**Requirements**:
1. **Separate Offset Store**: Track offsets independently
   ```sql
   CREATE TABLE consumer_offsets (
       consumer_id VARCHAR(255),
       message_group VARCHAR(255),
       last_processed_id BIGINT,
       last_processed_at TIMESTAMP WITH TIME ZONE,
       PRIMARY KEY (consumer_id, message_group)
   );
   ```

2. **Broker-Enforced Assignment**:
   - When fetching messages, check if consumer is still assigned
   - If not, reassign immediately
   - If consumer dies, reassign to another

3. **Heartbeat Mechanism**:
   - Consumers send heartbeats
   - Broker detects failure immediately
   - Reassigns that consumer's groups to others

4. **Modified Consumer Query**:
   ```sql
   SELECT id FROM outbox
   WHERE topic = $1
     AND status = 'PENDING'
     AND message_group = (
       SELECT message_group FROM consumer_assignments
       WHERE consumer_id = $2
         AND assigned_at > NOW() - INTERVAL '30 seconds'
     )
   ORDER BY created_at ASC
   LIMIT $3
   FOR UPDATE SKIP LOCKED
   ```

5. **Immediate Reassignment on Failure**:
   - Heartbeat timeout detected
   - Immediately reassign that consumer's groups
   - Next consumer picks up from stored offset

**Advantages**:
- No stop-the-world rebalancing
- Failure of one consumer doesn't pause others
- Simpler consumer logic
- Better fault tolerance
- More elegant than Kafka's approach

**Disadvantages**:
- More complex broker-side logic
- Requires offset tracking per consumer
- Requires immediate reassignment mechanism

---

### Option 3: Hybrid Approach (Recommended for PeeGeeQ)

Combine the best of both approaches:

1. **Configurable Mode**:
   - `mode: "throughput"` → Current behavior (any consumer grabs any message)
   - `mode: "ordered"` → Pulsar-like partition assignment (strict ordering per group)

2. **Gradual Adoption**:
   - Start with Kafka-like approach (simpler to implement)
   - Migrate to Pulsar-like approach (better fault tolerance)

3. **Per-Topic Configuration**:
   - Some topics use throughput mode (analytics, non-critical)
   - Some topics use ordered mode (financial, critical)

---

## Comparison: Current PeeGeeQ vs. Future Options

| Aspect | Current | Kafka-Like | Pulsar-Like |
|--------|---------|-----------|------------|
| Per-Group Ordering | ❌ | ✅ | ✅ |
| Throughput | ✅ High | ⚠️ Medium | ⚠️ Medium |
| Rebalancing Pause | N/A | ✅ Stop-the-world | ❌ Per-partition |
| Consumer Failure Impact | N/A | All pause | Only affected group |
| Operational Complexity | Low | Medium | High |
| Implementation Effort | N/A | Medium | High |
| Fault Tolerance | N/A | Good | Excellent |

---

## Recommendation

For PeeGeeQ's evolution:

1. **Short-term** (next release): Document the three patterns (already done) and recommend Pattern 3 (application-level partition routing) for users needing strict ordering

2. **Medium-term** (1-2 releases): Implement Kafka-like partition assignment as an opt-in feature for consumer groups

3. **Long-term** (future): Consider Pulsar-like broker-enforced assignment for better fault tolerance and operational simplicity

The Pulsar approach is architecturally superior, but Kafka-like is easier to implement and understand. Start with Kafka-like, migrate to Pulsar-like if operational complexity becomes a pain point.

